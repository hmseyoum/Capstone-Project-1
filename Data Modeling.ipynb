{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary modules/libraries\n",
    "\n",
    "import numpy as  np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from matplotlib import rcParams\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"talk\", font_scale=0.7)\n",
    "sns.set_palette(\"Greens_r\")\n",
    "#set_palette(\"Set1\", 8, .75) # makes plot lines red\n",
    "\n",
    "# Ignore  the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.preprocessing import Imputer # to impute missing data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# read the data\n",
    "df1 = pd.read_csv('house_price_prediction.csv')\n",
    "df2 = df1.copy()\n",
    "\n",
    "# change date col to datetime\n",
    "df2['date'] = pd.to_datetime(df2['date'])\n",
    "\n",
    "# change data types of 'waterfront' & 'condition' to categorical\n",
    "df2[['waterfront', 'condition']] = df1[['waterfront', 'condition']].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splittig statezip into state & zipcode features & removing statezip\n",
    "df2['state'] = df2['statezip'].apply(lambda x: x.split(' ')[0])\n",
    "df2['zipcode'] = df2['statezip'].apply(lambda x: int(x.split(' ')[1]))\n",
    "df2 = df2.drop('statezip', axis='columns')\n",
    "\n",
    "# creating total_sqft feature: sqft_above + sqft_lot\n",
    "df2['total_sqft'] = df2.sqft_living + df2.sqft_lot\n",
    "\n",
    "# creating month feature\n",
    "df2['month'] = pd.DatetimeIndex(df2['date']).month\n",
    "\n",
    "df3 = df2.copy()\n",
    "\n",
    "# creating a column of boolean arrays with 1s for when price is $0 & 0s for when price is not $0\n",
    "df3['price_is_zero'] = (df3['price'] == 0).astype(int)\n",
    "\n",
    "# creating a column of boolean arrays with 1s for when yr_renov is 0 & 0s for others\n",
    "df3['renov_date_is_zero'] = (df3['yr_renovated'] == 0).astype(int)\n",
    "\n",
    "# replacing 0s with the mean bedroom & bathroom values\n",
    "df3[\"bedrooms\"].replace({0: round(df3[\"bedrooms\"].mean(), 0)}, inplace=True)\n",
    "df3[\"bathrooms\"].replace({0: round(df3[\"bathrooms\"].mean(), 0)}, inplace=True)\n",
    "\n",
    "# removing price outliers\n",
    "df_no_outs = df3[df3.price < 10000000].copy()\n",
    "\n",
    "# creating a new dataframe with $0 prices removed to compare later\n",
    "df_no_zeros = df_no_outs[df_no_outs[\"price\"] != 0].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanprice_bedrooms = df_no_zeros.groupby('bedrooms')['price'].mean()\n",
    "medianprice_bedrooms = df_no_zeros.groupby('bedrooms')['price'].median()\n",
    "\n",
    "meansqft_bedroom = df_no_zeros.groupby(\"bedrooms\")[\"sqft_living\"].mean()\n",
    "mediansqft_bedroom = df_no_zeros.groupby(\"bedrooms\")[\"sqft_living\"].median()\n",
    "\n",
    "may_price = df_no_zeros[df_no_zeros.month==5]['price']\n",
    "june_price = df_no_zeros[df_no_zeros.month==6]['price']\n",
    "july_price = df_no_zeros[df_no_zeros.month==7]['price']\n",
    "\n",
    "def ecdf(data):\n",
    "    \"\"\"Compute ECDF for a one-dimensional array of measurements.\"\"\"\n",
    "    n = len(data)\n",
    "\n",
    "    x = np.sort(data)\n",
    "\n",
    "    y = np.arange(1, n+1) / n\n",
    "\n",
    "    return x, y\n",
    "\n",
    "# Compute ECDFs\n",
    "x_5, y_5 = ecdf(may_price)\n",
    "x_6, y_6 = ecdf(june_price)\n",
    "x_7, y_7 = ecdf(july_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "def feature_importance(X, y, model='reg'):\n",
    "    score_func = {'reg': f_regression}\n",
    "\n",
    "    # Score each of the features\n",
    "    bestfeatures = SelectKBest(score_func=score_func[model], k='all')\n",
    "    fit = bestfeatures.fit(X, y)\n",
    "\n",
    "    # Organize and return the scores\n",
    "    featureScores = pd.DataFrame([X.columns, fit.scores_]).T\n",
    "    featureScores.columns = ['Feature', 'Score']\n",
    "    return featureScores.sort_values('Score', ascending=False).set_index('Feature') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Test I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ho**: Prices of houses with many bedrooms and a few bedrooms are equal.  \n",
    "**Ha**: Prices of houses with many bedrooms and a few bedrooms are different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-test:\n",
    "I'll compare 2 groups: one with `bedrooms` greater than or equal to 4, and another with less than 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=20.612406446603853, pvalue=2.3169170411918228e-90)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "many_bedrooms = df_no_outs[df_no_outs['bedrooms'] >= 4]['price']\n",
    "few_bedrooms = df_no_outs[df_no_outs['bedrooms'] < 4]['price']\n",
    "stats.ttest_ind(many_bedrooms, few_bedrooms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result\n",
    "**t-stat**: 20.6  \n",
    "**p-val**: 2.3e-90 < 0.05  \n",
    "We reject the null hypothesis that houses with many bedrooms are priced around the same as houses with fewer bedrooms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Test II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ho**: Houses with waterfronts are priced the same as houses without waterfronts.  \n",
    "**Ha**: Houses with waterfronts are priced differently than houses without waterfronts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=14.441521571804724, pvalue=2.8817562762538972e-46)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waterfront = df_no_outs[df_no_outs['waterfront'] == 1]['price']\n",
    "no_waterfront = df_no_outs[df_no_outs['waterfront'] == 0]['price']\n",
    "stats.ttest_ind(waterfront, no_waterfront)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result\n",
    "**t-stat**: 14.4  \n",
    "**p-val**: 2.9e-46 < 0.05  \n",
    "We reject the null hypothesis that houses with waterfronts are priced the same as houses without waterfronts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_outs = df3[df3.price < 10000000]\n",
    "df_no_zeros = df_no_outs[df_no_outs[\"price\"] != 0]\n",
    "\n",
    "# dataframe with both zero prices and outliers.\n",
    "X = df3.drop(['price', 'date', 'street', 'city', 'state', 'country'], axis='columns')\n",
    "y = df3[\"price\"]\n",
    "# splitting data into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)\n",
    "\n",
    "# dataframe without price outliers\n",
    "X_no_outs = df_no_outs.drop(['price', 'date', 'street', 'city', 'state', 'country'], axis='columns')\n",
    "y_no_outs = df_no_outs[\"price\"]\n",
    "X_train_no_outs, X_test_no_outs, y_train_no_outs, y_test_no_outs = train_test_split(X_no_outs, y_no_outs, test_size = 0.3, random_state=42)\n",
    "\n",
    "# dataframe without $0 price values\n",
    "X_no_zeros = df_no_zeros.drop(['price', 'date', 'street', 'city', 'state', 'country'], axis='columns')\n",
    "y_no_zeros = df_no_zeros[\"price\"]\n",
    "X_train_no_zeros, X_test_no_zeros, y_train_no_zeros, y_test_no_zeros = train_test_split(X_no_zeros, y_no_zeros, test_size = 0.3, random_state=42)\n",
    "\n",
    "# dataframe with a parabolic transformation on the `bedrooms` feature\n",
    "X_transformed = df_no_zeros.drop(['price', 'date', 'street', 'city', 'state', 'country'], axis='columns')\n",
    "X_transformed['bedrooms_squared'] = X['bedrooms']**2  # parabolic transformation of bedrooms\n",
    "y_transformed = df_no_zeros['price']\n",
    "X_train_transformed, X_test_transformed, y_train_transformed, y_test_transformed = train_test_split(X_transformed, y_transformed, test_size = 0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 training sets and test sets\n",
    "# 3 algorithms\n",
    "\n",
    "set1 = [X_train, X_test, y_train, y_test]\n",
    "set2 = [X_train_no_outs, X_test_no_outs, y_train_no_outs, y_test_no_outs]\n",
    "set3 = [X_train_no_zeros, X_test_no_zeros, y_train_no_zeros, y_test_no_zeros]\n",
    "set4 = [X_train_transformed, X_test_transformed, y_train_transformed, y_test_transformed]\n",
    "\n",
    "train_test_sets = [set1, set2, set3, set4]\n",
    "\n",
    "algorithms = [LinearRegression, Ridge, RandomForestRegressor]\n",
    "names = ['linear regression', 'ridge regression ', 'random forest    ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_train_error(X_train, y_train, model):\n",
    "    '''returns in-sample error for already fit model.'''\n",
    "    y_pred = model.predict(X_train)\n",
    "    mse = mean_squared_error(y_train, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "    \n",
    "def calc_test_error(X_test, y_test, model):\n",
    "    '''returns out-of-sample error for already fit model.'''\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "    \n",
    "def calc_metrics(X_train, y_train, X_test, y_test, model):\n",
    "    '''fits model and returns the RMSE for in-sample error and out-of-sample error, and their accuracy score'''\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    train_error = calc_train_error(X_train, y_train, model)\n",
    "    test_error = calc_test_error(X_test, y_test, model)\n",
    "    \n",
    "    train_score = model.score(X_train, y_train)\n",
    "    test_score = model.score(X_test, y_test)\n",
    "    return train_error, test_error, train_score, test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing alpha for Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "alpha: 0.0001 | test_score: 0.06777947773383897 | test_error: 820661.0540042502 | test/train error: 3.4\n",
      "alpha: 0.001 | test_score: 0.0677794248900705 | test_error: 820661.0772642092 | test/train error: 3.4\n",
      "alpha: 0.01 | test_score: 0.06777897974205749 | test_error: 820661.273202603 | test/train error: 3.4\n",
      "alpha: 0.1 | test_score: 0.06777928636739461 | test_error: 820661.1382369914 | test/train error: 3.4\n",
      "alpha: 1 | test_score: 0.06782834917275116 | test_error: 820639.5422421335 | test/train error: 3.4\n",
      "alpha: 10.0 | test_score: 0.0676668767071551 | test_error: 820710.6155000443 | test/train error: 3.4\n",
      "\n",
      "alpha: 0.0001 | test_score: 0.5653871733995002 | test_error: 242953.0513279551 | test/train error: 1.0\n",
      "alpha: 0.001 | test_score: 0.5653873191173242 | test_error: 242953.01059906863 | test/train error: 1.0\n",
      "alpha: 0.01 | test_score: 0.5653888828896426 | test_error: 242952.57351617725 | test/train error: 1.0\n",
      "alpha: 0.1 | test_score: 0.5654117033213278 | test_error: 242946.1949915955 | test/train error: 1.0\n",
      "alpha: 1 | test_score: 0.5656890507214902 | test_error: 242868.6604033432 | test/train error: 1.0\n",
      "alpha: 10.0 | test_score: 0.5643549143000446 | test_error: 243241.40195248876 | test/train error: 1.0\n",
      "\n",
      "alpha: 0.0001 | test_score: 0.5953517271705626 | test_error: 236687.2803880412 | test/train error: 1.0\n",
      "alpha: 0.001 | test_score: 0.5953516994874801 | test_error: 236687.28848424947 | test/train error: 1.0\n",
      "alpha: 0.01 | test_score: 0.5953514286883995 | test_error: 236687.36768227073 | test/train error: 1.0\n",
      "alpha: 0.1 | test_score: 0.5953489589398709 | test_error: 236688.08998481612 | test/train error: 1.0\n",
      "alpha: 1 | test_score: 0.5953049326209416 | test_error: 236700.96555024775 | test/train error: 1.0\n",
      "alpha: 10.0 | test_score: 0.5938836077082867 | test_error: 237116.25860886744 | test/train error: 1.0\n",
      "\n",
      "alpha: 0.0001 | test_score: 0.595484903834457 | test_error: 236648.32826779856 | test/train error: 1.0\n",
      "alpha: 0.001 | test_score: 0.5954848388116147 | test_error: 236648.3472875415 | test/train error: 1.0\n",
      "alpha: 0.01 | test_score: 0.5954842160668711 | test_error: 236648.52944568018 | test/train error: 1.0\n",
      "alpha: 0.1 | test_score: 0.5954794964288515 | test_error: 236649.90997541894 | test/train error: 1.0\n",
      "alpha: 1 | test_score: 0.5954292749675731 | test_error: 236664.5996330863 | test/train error: 1.0\n",
      "alpha: 10.0 | test_score: 0.5939778191898639 | test_error: 237088.75372336322 | test/train error: 1.0\n"
     ]
    }
   ],
   "source": [
    "alphas = [1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1]\n",
    "\n",
    "for sets in train_test_sets:\n",
    "    print('')\n",
    "    for alpha in alphas:\n",
    "        model = Ridge(alpha = alpha)\n",
    "        train_error, test_error, train_score, test_score = calc_metrics(sets[0], sets[2],  sets[1], sets[3], model)\n",
    "        print('alpha: {} | test_score: {} | test_error: {} | test/train error: {}'.format(alpha, test_score, test_error, round(test_error/train_error, 1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the 1st & 2nd datasets (df3 & df_no_outs), the best alpha seems to be alpha=1.  \n",
    "For the 3rd and 4th datasets (df_no_zeros & df_transformed), alpha=0.0001 seems best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing parameters for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "max_depth: None | n_estimators: 1 | test_score: -0.019 | test_error: 858049.733 | test/train error: 4.3\n",
      "max_depth: None | n_estimators: 10 | test_score: 0.074 | test_error: 817934.311 | test/train error: 7.4\n",
      "max_depth: None | n_estimators: 100 | test_score: 0.079 | test_error: 815705.387 | test/train error: 9.3\n",
      "max_depth: None | n_estimators: 200 | test_score: 0.079 | test_error: 815563.206 | test/train error: 9.4\n",
      "max_depth: 10 | n_estimators: 1 | test_score: -0.014 | test_error: 856040.201 | test/train error: 3.9\n",
      "max_depth: 10 | n_estimators: 10 | test_score: 0.074 | test_error: 817735.444 | test/train error: 5.5\n",
      "max_depth: 10 | n_estimators: 100 | test_score: 0.077 | test_error: 816799.482 | test/train error: 6.0\n",
      "max_depth: 10 | n_estimators: 200 | test_score: 0.077 | test_error: 816705.971 | test/train error: 6.0\n",
      "max_depth: 100 | n_estimators: 1 | test_score: 0.023 | test_error: 840353.283 | test/train error: 3.8\n",
      "max_depth: 100 | n_estimators: 10 | test_score: 0.067 | test_error: 820950.04 | test/train error: 8.3\n",
      "max_depth: 100 | n_estimators: 100 | test_score: 0.078 | test_error: 816367.113 | test/train error: 9.6\n",
      "max_depth: 100 | n_estimators: 200 | test_score: 0.078 | test_error: 816170.453 | test/train error: 9.3\n",
      "\n",
      "max_depth: None | n_estimators: 1 | test_score: 0.135 | test_error: 342707.774 | test/train error: 1.7\n",
      "max_depth: None | n_estimators: 10 | test_score: 0.584 | test_error: 237619.232 | test/train error: 2.1\n",
      "max_depth: None | n_estimators: 100 | test_score: 0.652 | test_error: 217508.757 | test/train error: 2.4\n",
      "max_depth: None | n_estimators: 200 | test_score: 0.651 | test_error: 217573.683 | test/train error: 2.4\n",
      "max_depth: 10 | n_estimators: 1 | test_score: 0.338 | test_error: 299920.576 | test/train error: 1.4\n",
      "max_depth: 10 | n_estimators: 10 | test_score: 0.575 | test_error: 240129.029 | test/train error: 1.6\n",
      "max_depth: 10 | n_estimators: 100 | test_score: 0.639 | test_error: 221470.051 | test/train error: 1.6\n",
      "max_depth: 10 | n_estimators: 200 | test_score: 0.632 | test_error: 223582.373 | test/train error: 1.6\n",
      "max_depth: 100 | n_estimators: 1 | test_score: 0.291 | test_error: 310354.6 | test/train error: 1.3\n",
      "max_depth: 100 | n_estimators: 10 | test_score: 0.611 | test_error: 229853.81 | test/train error: 1.8\n",
      "max_depth: 100 | n_estimators: 100 | test_score: 0.653 | test_error: 217157.809 | test/train error: 2.4\n",
      "max_depth: 100 | n_estimators: 200 | test_score: 0.665 | test_error: 213326.2 | test/train error: 2.3\n",
      "\n",
      "max_depth: None | n_estimators: 1 | test_score: 0.151 | test_error: 342888.428 | test/train error: 1.7\n",
      "max_depth: None | n_estimators: 10 | test_score: 0.568 | test_error: 244563.395 | test/train error: 2.7\n",
      "max_depth: None | n_estimators: 100 | test_score: 0.649 | test_error: 220432.523 | test/train error: 2.6\n",
      "max_depth: None | n_estimators: 200 | test_score: 0.638 | test_error: 223787.227 | test/train error: 2.7\n",
      "max_depth: 10 | n_estimators: 1 | test_score: 0.082 | test_error: 356448.046 | test/train error: 1.8\n",
      "max_depth: 10 | n_estimators: 10 | test_score: 0.569 | test_error: 244313.18 | test/train error: 1.8\n",
      "max_depth: 10 | n_estimators: 100 | test_score: 0.635 | test_error: 224747.901 | test/train error: 1.7\n",
      "max_depth: 10 | n_estimators: 200 | test_score: 0.625 | test_error: 227953.486 | test/train error: 1.8\n",
      "max_depth: 100 | n_estimators: 1 | test_score: 0.307 | test_error: 309661.049 | test/train error: 1.5\n",
      "max_depth: 100 | n_estimators: 10 | test_score: 0.604 | test_error: 234049.066 | test/train error: 2.3\n",
      "max_depth: 100 | n_estimators: 100 | test_score: 0.649 | test_error: 220505.099 | test/train error: 2.5\n",
      "max_depth: 100 | n_estimators: 200 | test_score: 0.634 | test_error: 224993.645 | test/train error: 2.7\n",
      "\n",
      "max_depth: None | n_estimators: 1 | test_score: 0.292 | test_error: 313023.28 | test/train error: 1.8\n",
      "max_depth: None | n_estimators: 10 | test_score: 0.622 | test_error: 228654.493 | test/train error: 2.3\n",
      "max_depth: None | n_estimators: 100 | test_score: 0.645 | test_error: 221710.793 | test/train error: 2.6\n",
      "max_depth: None | n_estimators: 200 | test_score: 0.638 | test_error: 223777.751 | test/train error: 2.7\n",
      "max_depth: 10 | n_estimators: 1 | test_score: 0.024 | test_error: 367585.048 | test/train error: 1.8\n",
      "max_depth: 10 | n_estimators: 10 | test_score: 0.553 | test_error: 248886.563 | test/train error: 1.8\n",
      "max_depth: 10 | n_estimators: 100 | test_score: 0.622 | test_error: 228895.021 | test/train error: 1.8\n",
      "max_depth: 10 | n_estimators: 200 | test_score: 0.617 | test_error: 230266.357 | test/train error: 1.8\n",
      "max_depth: 100 | n_estimators: 1 | test_score: 0.151 | test_error: 342833.47 | test/train error: 1.7\n",
      "max_depth: 100 | n_estimators: 10 | test_score: 0.64 | test_error: 223096.72 | test/train error: 2.1\n",
      "max_depth: 100 | n_estimators: 100 | test_score: 0.631 | test_error: 225926.332 | test/train error: 2.7\n",
      "max_depth: 100 | n_estimators: 200 | test_score: 0.64 | test_error: 223224.467 | test/train error: 2.7\n"
     ]
    }
   ],
   "source": [
    "max_depths = [None, 10, 100]\n",
    "n_estimators = [1, 10, 100, 200]\n",
    "\n",
    "for sets in train_test_sets:\n",
    "    print('')\n",
    "    for max_depth in max_depths:\n",
    "        for n in n_estimators:\n",
    "            model = RandomForestRegressor(max_depth=max_depth , n_estimators=n)\n",
    "            train_error, test_error, train_score, test_score = calc_metrics(sets[0], sets[2],  sets[1], sets[3], model)\n",
    "            train_error, test_error, train_score, test_score = round(train_error,3), round(test_error,3), round(train_score,3), round(test_score,3)\n",
    "            print('max_depth: {} | n_estimators: {} | test_score: {} | test_error: {} | test/train error: {}'.format(max_depth, n, test_score, test_error, round(test_error/train_error, 1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the best values for parameters `max_depth` & `n_estimators` are None & 200 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_train_score = []\n",
    "lr_test_score = []\n",
    "lr_train_error = []\n",
    "lr_test_error = []\n",
    "\n",
    "ridge_train_score = []\n",
    "ridge_test_score = []\n",
    "ridge_train_error = []\n",
    "ridge_test_error = []\n",
    "\n",
    "rf_train_score = []\n",
    "rf_test_score = []\n",
    "rf_train_error = []\n",
    "rf_test_error = []\n",
    "\n",
    "for sets in train_test_sets:\n",
    "    \n",
    "    # linear regression\n",
    "    \n",
    "    model = algorithms[0]()\n",
    "    train_error, test_error, train_score, test_score = calc_metrics(sets[0], sets[2], sets[1], sets[3], model)\n",
    "    train_error, test_error, train_score, test_score = round(train_error, 3), round(test_error, 3), round(train_score, 3), round(test_score, 3)\n",
    "    \n",
    "    lr_train_error.append(train_error)\n",
    "    lr_test_error.append(test_error)\n",
    "    lr_train_score.append(train_score)\n",
    "    lr_test_score.append(test_score)\n",
    "    \n",
    "    # ridge regression\n",
    "    \n",
    "    model = algorithms[1](alpha=0.0001)\n",
    "    train_error, test_error, train_score, test_score = calc_metrics(sets[0], sets[2], sets[1], sets[3], model)\n",
    "    train_error, test_error, train_score, test_score = round(train_error, 3), round(test_error, 3), round(train_score, 3), round(test_score, 3)\n",
    "    \n",
    "    ridge_train_error.append(train_error)\n",
    "    ridge_test_error.append(test_error)\n",
    "    ridge_train_score.append(train_score)\n",
    "    ridge_test_score.append(test_score)\n",
    "    \n",
    "    # random forest\n",
    "    \n",
    "    model = algorithms[2](max_depth=None, n_estimators=200)\n",
    "    train_error, test_error, train_score, test_score = calc_metrics(sets[0], sets[2], sets[1], sets[3], model)\n",
    "    train_error, test_error, train_score, test_score = round(train_error, 3), round(test_error, 3), round(train_score, 3), round(test_score, 3)\n",
    "    \n",
    "    rf_train_error.append(train_error)\n",
    "    rf_test_error.append(test_error)\n",
    "    rf_train_score.append(train_score)\n",
    "    rf_test_score.append(test_score)\n",
    "    \n",
    "    '''\n",
    "    print('train error: {} | test error: {}'.format(train_error, test_error))\n",
    "    print('test/train: {}'.format(round(test_error/train_error, 1)))\n",
    "    print(names[0], \"R^2: {}, RMSE: {}\".format(test_score, rmse))\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>test_score</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_error</th>\n",
       "      <th>train_error</th>\n",
       "      <th>price outs removed</th>\n",
       "      <th>description</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>random forest</td>\n",
       "      <td>0.649</td>\n",
       "      <td>0.942</td>\n",
       "      <td>218414.099</td>\n",
       "      <td>89771.290</td>\n",
       "      <td>yes</td>\n",
       "      <td>no transformations</td>\n",
       "      <td>all except date, price, street, city, state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>random forest</td>\n",
       "      <td>0.644</td>\n",
       "      <td>0.946</td>\n",
       "      <td>221871.023</td>\n",
       "      <td>85370.755</td>\n",
       "      <td>yes</td>\n",
       "      <td>parabolic transformation</td>\n",
       "      <td>all except date, price, street, city, state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>random forest</td>\n",
       "      <td>0.635</td>\n",
       "      <td>0.949</td>\n",
       "      <td>224702.115</td>\n",
       "      <td>83128.174</td>\n",
       "      <td>yes</td>\n",
       "      <td>zero prices removed</td>\n",
       "      <td>all except date, price, street, city, state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>linear regression</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.572</td>\n",
       "      <td>236687.279</td>\n",
       "      <td>239639.587</td>\n",
       "      <td>yes</td>\n",
       "      <td>zero prices removed</td>\n",
       "      <td>all except date, price, street, city, state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>linear regression</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.572</td>\n",
       "      <td>236648.326</td>\n",
       "      <td>239620.430</td>\n",
       "      <td>yes</td>\n",
       "      <td>parabolic transformation</td>\n",
       "      <td>all except date, price, street, city, state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ridge regression</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.572</td>\n",
       "      <td>236687.280</td>\n",
       "      <td>239639.587</td>\n",
       "      <td>yes</td>\n",
       "      <td>zero prices removed</td>\n",
       "      <td>all except date, price, street, city, state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ridge regression</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.572</td>\n",
       "      <td>236648.328</td>\n",
       "      <td>239620.430</td>\n",
       "      <td>yes</td>\n",
       "      <td>parabolic transformation</td>\n",
       "      <td>all except date, price, street, city, state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>linear regression</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.577</td>\n",
       "      <td>242953.056</td>\n",
       "      <td>241694.255</td>\n",
       "      <td>yes</td>\n",
       "      <td>no transformations</td>\n",
       "      <td>all except date, price, street, city, state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ridge regression</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.577</td>\n",
       "      <td>242953.051</td>\n",
       "      <td>241694.255</td>\n",
       "      <td>yes</td>\n",
       "      <td>no transformations</td>\n",
       "      <td>all except date, price, street, city, state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>random forest</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.948</td>\n",
       "      <td>816809.782</td>\n",
       "      <td>86464.164</td>\n",
       "      <td>no</td>\n",
       "      <td>no transformations</td>\n",
       "      <td>all except date, price, street, city, state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>linear regression</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.594</td>\n",
       "      <td>820661.051</td>\n",
       "      <td>241949.171</td>\n",
       "      <td>no</td>\n",
       "      <td>no transformations</td>\n",
       "      <td>all except date, price, street, city, state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ridge regression</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.594</td>\n",
       "      <td>820661.054</td>\n",
       "      <td>241949.171</td>\n",
       "      <td>no</td>\n",
       "      <td>no transformations</td>\n",
       "      <td>all except date, price, street, city, state</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model  test_score  train_score  test_error  train_error  \\\n",
       "10      random forest       0.649        0.942  218414.099    89771.290   \n",
       "12      random forest       0.644        0.946  221871.023    85370.755   \n",
       "11      random forest       0.635        0.949  224702.115    83128.174   \n",
       "3   linear regression       0.595        0.572  236687.279   239639.587   \n",
       "4   linear regression       0.595        0.572  236648.326   239620.430   \n",
       "7    ridge regression       0.595        0.572  236687.280   239639.587   \n",
       "8    ridge regression       0.595        0.572  236648.328   239620.430   \n",
       "2   linear regression       0.565        0.577  242953.056   241694.255   \n",
       "6    ridge regression       0.565        0.577  242953.051   241694.255   \n",
       "9       random forest       0.077        0.948  816809.782    86464.164   \n",
       "1   linear regression       0.068        0.594  820661.051   241949.171   \n",
       "5    ridge regression       0.068        0.594  820661.054   241949.171   \n",
       "\n",
       "   price outs removed               description  \\\n",
       "10                yes        no transformations   \n",
       "12                yes  parabolic transformation   \n",
       "11                yes       zero prices removed   \n",
       "3                 yes       zero prices removed   \n",
       "4                 yes  parabolic transformation   \n",
       "7                 yes       zero prices removed   \n",
       "8                 yes  parabolic transformation   \n",
       "2                 yes        no transformations   \n",
       "6                 yes        no transformations   \n",
       "9                  no        no transformations   \n",
       "1                  no        no transformations   \n",
       "5                  no        no transformations   \n",
       "\n",
       "                                       features  \n",
       "10  all except date, price, street, city, state  \n",
       "12  all except date, price, street, city, state  \n",
       "11  all except date, price, street, city, state  \n",
       "3   all except date, price, street, city, state  \n",
       "4   all except date, price, street, city, state  \n",
       "7   all except date, price, street, city, state  \n",
       "8   all except date, price, street, city, state  \n",
       "2   all except date, price, street, city, state  \n",
       "6   all except date, price, street, city, state  \n",
       "9   all except date, price, street, city, state  \n",
       "1   all except date, price, street, city, state  \n",
       "5   all except date, price, street, city, state  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features1 = 'all except date, price, street, city, state'\n",
    "features2 = 'all except date, price, street, city, state'\n",
    "features3 = 'all except date, price, street, city, state'\n",
    "features4 = 'all except date, price, street, city, state'\n",
    "features5 = 'all except date, price, street, city, state'\n",
    "features6 = 'all except date, price, street, city, state'\n",
    "features7 = 'all except date, price, street, city, state'\n",
    "features8 = 'all except date, price, street, city, state'\n",
    "features9 = 'all except date, price, street, city, state'\n",
    "features10 = 'all except date, price, street, city, state'\n",
    "features11 = 'all except date, price, street, city, state'\n",
    "features12 = 'all except date, price, street, city, state'\n",
    "\n",
    "content = {'features': [features1, features2, features3, features4, features5, features6, \n",
    "                        features7, features8, features9, features10, features11, features12], \n",
    "           'model': ['linear regression', 'linear regression', 'linear regression', 'linear regression',\n",
    "                     'ridge regression', 'ridge regression', 'ridge regression', 'ridge regression', \n",
    "                     'random forest', 'random forest', 'random forest', 'random forest'], \n",
    "           'test_error': [lr_test_error[0], lr_test_error[1], lr_test_error[2], lr_test_error[3], \n",
    "                    ridge_test_error[0], ridge_test_error[1], ridge_test_error[2], ridge_test_error[3], \n",
    "                    rf_test_error[0], rf_test_error[1], rf_test_error[2], rf_test_error[3]],\n",
    "           'train_error': [lr_train_error[0], lr_train_error[1], lr_train_error[2], lr_train_error[3], \n",
    "                    ridge_train_error[0], ridge_train_error[1], ridge_train_error[2], ridge_train_error[3], \n",
    "                    rf_train_error[0], rf_train_error[1], rf_train_error[2], rf_train_error[3]],\n",
    "           'test_score': [lr_test_score[0], lr_test_score[1], lr_test_score[2], lr_test_score[3], \n",
    "                     ridge_test_score[0], ridge_test_score[1], ridge_test_score[2], ridge_test_score[3],  \n",
    "                     rf_test_score[0], rf_test_score[1], rf_test_score[2], rf_test_score[3]],\n",
    "           'train_score': [lr_train_score[0], lr_train_score[1], lr_train_score[2], lr_train_score[3],\n",
    "                             ridge_train_score[0], ridge_train_score[1], ridge_train_score[2], ridge_train_score[3],\n",
    "                             rf_train_score[0], rf_train_score[1], rf_train_score[2], rf_train_score[3]],\n",
    "           'description': ['no transformations', 'no transformations', 'zero prices removed', 'parabolic transformation', \n",
    "                           'no transformations', 'no transformations', 'zero prices removed', 'parabolic transformation', \n",
    "                           'no transformations', 'no transformations', 'zero prices removed', 'parabolic transformation'],\n",
    "           'price outs removed': ['no', 'yes', 'yes', 'yes', 'no', 'yes', 'yes', 'yes', 'no', 'yes', 'yes', 'yes']}\n",
    "\n",
    "table = pd.DataFrame(content, columns=['model', 'test_score', 'train_score', 'test_error', 'train_error', 'price outs removed', 'description', 'features'], index=[1,2,3,4,5,6,7,8,9,10,11,12])\n",
    "table.sort_values('test_score', ascending=False)\n",
    "\n",
    "# include hyperparameters, test_RMSE, training_RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model description:\n",
    "* \n",
    "* \n",
    "* \n",
    "* \n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the best regressor is the Random Forest, with 62% accuracy. It makes sense that the data frame with price outliers and zeros removed had the best performing model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CI around my prediction accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate standard error RMSE\n",
    ".."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[816744.468, 230769.646, 246093.763, 231884.757]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RMSE == standard deviation of errors\n",
    "rf_test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_sd = min(rf_test_error)\n",
    "two_sd = 2*one_sd\n",
    "three_sd = 3*one_sd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 68% Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can be 68% confident that the error in the home price predictions will be between -230769.646 and 230769.646.\n"
     ]
    }
   ],
   "source": [
    "min_error = 0 - one_sd\n",
    "max_error = 0 + one_sd\n",
    "\n",
    "print('I can be 68% confident that the error in the home price predictions will be between {} and {}.'.format(min_error, max_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 95% Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can be 95% confident that the error in the home price predictions will be between -461539.292 and 461539.292.\n"
     ]
    }
   ],
   "source": [
    "min_error = 0 - two_sd\n",
    "max_error = 0 + two_sd\n",
    "\n",
    "print('I can be 95% confident that the error in the home price predictions will be between {} and {}.'.format(min_error, max_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following analysis is based on my 68% confidence that the error in the home price predictions is between -218832.738 and 218832.738:\n",
    "\n",
    "If I've predicted a house to be $\\$$400,000 and someone were to buy the house at that price, the worst loss would be \\\\$181,168 (400,000-218,832), and the highest gain would be \\\\$618,832 (400,000+218,832).\n",
    "\n",
    "Let's say my boss wants to buy a house at below my estimate in the hopes of flipping it, i.e., selling the house at around double the price at which he acquired it. With a house that I have estimated to be \\\\$400,000, if he purchases it at \\\\$300,000 for example, 3 things can happen:\n",
    "\n",
    "1. My estimation is correct and he will make a profit of \\\\$100,000 by just acquiring it, and could make another \\\\$300,000 by flipping it, making that a total profit of \\\\$400,000. \n",
    "2. The house is actually worth \\\\$181,168 and he ends up buying at a higher price than what the house is worth, losing \\\\$118,832 on the purchase.\n",
    "3. The house is actually worth \\\\$618,832 and he ends up buying \\\\$318,832 below the house value, resulting in a 106%  profit just from acquiring it. Meaning, after investing \\\\$300,000, and expecting to make a profit of \\\\$200,000-\\\\$300,000 by flipping it, he ends up making a profit of \\\\$318,832 instead by just acquiring it, and another \\\\$300,000 by flipping it, making that a total of \\\\$618,832 of profit.\n",
    "\n",
    "Now my boss purchased 100 homes at \\$1000 less than my estimated price of \\\\$400,000:\n",
    "\n",
    "His **average gain** would be \\\\$100,000.\n",
    "\n",
    "His **worst loss** -- with actual price: \\\\$181,168 (i.e., 400,000-218,832), purchase price: \\\\$399,000  \n",
    "\\\\$21,783,200\n",
    "\n",
    "    difference between purchase price & actual price: 399,000 - 181,168 = 217,832\n",
    "    for 100 homes: 217,832*100 = $21,783,200\n",
    "\n",
    "His **highest gain** -- with actual price: \\\\$618,832 (i.e., 400,000+218,832), purchase price: \\\\$399,000  \n",
    "\\\\$21,983,200\n",
    "\n",
    "    difference between actual price & purchase price: 618,832 - 399,000 = 219,832\n",
    "    for 100 homes 100: 219,832*100 = $21,983,200\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
